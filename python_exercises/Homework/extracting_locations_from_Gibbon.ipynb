{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5z4dLpxV1RR"
   },
   "source": [
    "# Extracting location names and data from Gibbon\n",
    "\n",
    "In this note book we will put together and practice a lot of skills we have learned so far this term. Starting with just the raw text files from Gibbon's Decline and Fall we will create a DataFrame containing location names, location counts, and location data.\n",
    "\n",
    "The code in this notebook may seem complex, but if you read through it carefully, you will likely understand what most of the code is doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePnMpLjAW7Is"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bb1Ifmxs1P7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/Max/miniconda3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (1.26.1)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (4.24.4)\n",
      "Requirement already satisfied: requests in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (2.29.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/Max/miniconda3/lib/python3.11/site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.2)\n",
      "Requirement already satisfied: jinja2 in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/Max/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Max/miniconda3/lib/python3.11/site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Max/miniconda3/lib/python3.11/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Max/miniconda3/lib/python3.11/site-packages (from requests->stanza) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Max/miniconda3/lib/python3.11/site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Max/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Max/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries. The \"%%capture\" stops the notebook from printing\n",
    "# out all the insall output. Remove if you need to trouble shoot.\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f32agA_TLXZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=470a3f0123b6045e15e3585aa050461c317b37775f043243efb341c6fa54da6c\n",
      "  Stored in directory: /Users/Max/Library/Caches/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries. The \"%%capture\" stops the notebook from printing\n",
    "# out all the insall output. Remove if you need to trouble shoot.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2OMgFur_1HV9"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import json\n",
    "import wget\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4sDsirAU1RO"
   },
   "source": [
    "## NLP pipeline\n",
    "Now that all the necessary libraries have been installed and imported into our project, we need to set up our nlp pipeline. We will use [Stanza](https://stanfordnlp.github.io/stanza/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZtcNgrIo11p7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 21:18:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83534336f0cf44b686822f66baae148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 21:18:27 INFO: Loading these models for language: en (English):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | combined         |\n",
      "| ner       | ontonotes_charlm |\n",
      "================================\n",
      "\n",
      "2023-11-03 21:18:27 INFO: Using device: cpu\n",
      "2023-11-03 21:18:27 INFO: Loading: tokenize\n",
      "2023-11-03 21:18:27 INFO: Loading: ner\n",
      "2023-11-03 21:18:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# load stanza nlp pipeline that tokenizes and performs Named Entity Recognition\n",
    "nlp_ner= stanza.Pipeline(lang='en', processors='tokenize, ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RuRpqCrUkfV"
   },
   "source": [
    "## Load text data\n",
    "If you are using a **Colab Notebook** you will need to run the cell below to get the text files.\n",
    "\n",
    "Otherwise, you should have all of the text files for Gibbon's _Decline and Fall of the Roman Empire_ already downloaded from Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_cJWBpn1NbP"
   },
   "outputs": [],
   "source": [
    "# load text files, Colab only\n",
    "! git clone https://github.com/jdeen33/Gibbon_text.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asRByu0XYpxM"
   },
   "source": [
    "## Extract location infromation from text file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-mG-vRFd2INL"
   },
   "outputs": [],
   "source": [
    "# create function that will take a text string as input and return a dictionary\n",
    "# with locations and location counts from the text string\n",
    "def get_locations_from_text(text):\n",
    "    locations_dict = {}\n",
    "    doc = nlp_ner(text)\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            if token.ner == 'S-GPE':\n",
    "                if not token.text in locations_dict.keys():\n",
    "                    locations_dict[token.text] = 1\n",
    "                else:\n",
    "                    locations_dict[token.text] += 1\n",
    "            else:\n",
    "                continue\n",
    "    return locations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLuZZ8pQWhzZ"
   },
   "source": [
    "You will need to choose which chapter you would like to extract locations from. For this example I will use Chapter 16.\n",
    "\n",
    "For **Colab** it will look something like this:\n",
    "`/content/Gibbon_text/gibbon_decline_volume1_chap16.txt`\n",
    "\n",
    "For **Jupyter** it will look something like this:\n",
    "`../text/gibbon_decline_and_fall/gibbon_decline_volume1_chap16.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I8UTBqCo1ZfE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# identify the path to the text file you want to use\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m path_to_file \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m/\u001b[39mgibbon_decline_and_fall\u001b[38;5;241m/\u001b[39mgibbon_decline_volume1_chap16\u001b[38;5;241m.\u001b[39mtxt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# identify the path to the text file you want to use\n",
    "path_to_file = text/gibbon_decline_and_fall/gibbon_decline_volume1_chap16.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48J1k2Y41qFf"
   },
   "outputs": [],
   "source": [
    "# read text from text file\n",
    "with open(path_to_file, encoding='utf-8', mode='r') as f:\n",
    "       text  = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNQh8jQt5a0y"
   },
   "outputs": [],
   "source": [
    "# apply function to get locations and location counts\n",
    "# this will take a few minutes\n",
    "locations = get_locations_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZ4QZ9hYCMtp"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgiG4AhrYc4r"
   },
   "outputs": [],
   "source": [
    "# you may want to save the locations dictionary\n",
    "path = './' # <-- Path of your choosing\n",
    "file_name = 'locations_data.json'\n",
    "with open(file_name, encoding='utf-8', mode='w') as f:\n",
    "    json.dump(locations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EbPGEP7-iCU"
   },
   "outputs": [],
   "source": [
    "# convert dictionary to dataframe for easier processing\n",
    "location_count_df = pd.DataFrame.from_dict(locations, orient='index').reset_index().rename(columns={'index':'place_name', 0:'count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o6gwxI-Kqe-"
   },
   "outputs": [],
   "source": [
    "# preview DataFrame\n",
    "location_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4eApj2FY5Bh"
   },
   "source": [
    "## Load data from Pleiades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZgTGV3n_Wds"
   },
   "outputs": [],
   "source": [
    "# data from Pleiades, thanks to Peter Nadel!\n",
    "if not os.path.isfile('places.csv'):  # checkin to see if we have this file or not\n",
    "    wget.download('https://raw.githubusercontent.com/pnadelofficial/FallDHCourseMaterials/main/places.csv')\n",
    "if not os.path.isfile('names.csv'):\n",
    "    wget.download('https://raw.githubusercontent.com/pnadelofficial/FallDHCourseMaterials/main/names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDkKvWOmLwYB"
   },
   "outputs": [],
   "source": [
    "# load and preview places DataFrame\n",
    "places_df = pd.read_csv('places.csv')\n",
    "places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUfxtGYVL7WQ"
   },
   "outputs": [],
   "source": [
    "# load and preview names DataFrame\n",
    "names_df = pd.read_csv('names.csv')\n",
    "names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBd8KpGkMJJa"
   },
   "outputs": [],
   "source": [
    "# quick example: find 'Roma' in places DataFrame\n",
    "places_df.loc[places_df['title'] == 'Roma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sZJttfMMddx"
   },
   "outputs": [],
   "source": [
    "# quick example: find 'Rome' in names DataFrame\n",
    "names_df.loc[names_df['romanized_form_1'] == 'Rome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNRABy-1cQeD"
   },
   "source": [
    "## Extract data from Pleiades data\n",
    "For each location in we identified from the text, we will extract extract the longitude, latitude, and a description. First we need to find each location in the Pleiades data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liMoqMCrMuj0"
   },
   "outputs": [],
   "source": [
    "def get_pleiades_id(location):\n",
    "    \"\"\"\n",
    "    Iterates through all of the possible names in the names.csv file\n",
    "    Returns None if no matched names\n",
    "    \"\"\"\n",
    "    name_row = names_df.loc[names_df['attested_form'] == location]\n",
    "    if len(name_row) == 1:\n",
    "        return int(name_row.place_id.iloc[0])\n",
    "    else:\n",
    "        name_row = names_df.loc[names_df['romanized_form_1'] == location]\n",
    "        if len(name_row) == 1:\n",
    "            return int(name_row.place_id.iloc[0])\n",
    "        else:\n",
    "            name_row = names_df.loc[names_df['romanized_form_2'] == location]\n",
    "            if len(name_row) == 1:\n",
    "                return int(name_row.place_id.iloc[0])\n",
    "            else:\n",
    "                name_row = names_df.loc[names_df['romanized_form_3'] == location]\n",
    "                if len(name_row) == 1:\n",
    "                    return int(name_row.place_id.iloc[0])\n",
    "                else:\n",
    "                    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7Myaj-ANa1K"
   },
   "outputs": [],
   "source": [
    "# apply the above founction to each row in our location count DataFrame and then\n",
    "# add a new colum with the Pleiades id\n",
    "location_count_df['pleiades_id'] = location_count_df['place_name'].apply(get_pleiades_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot1yzqFXN15c"
   },
   "outputs": [],
   "source": [
    "# preview new location count DataFrame.\n",
    "# the NaN means we were unable to find the location in the Pleiades data.\n",
    "location_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRvsWJAmN6H8"
   },
   "outputs": [],
   "source": [
    "# we can drop the rows with NaN values\n",
    "location_count_df = location_count_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw5FlATnO2VS"
   },
   "outputs": [],
   "source": [
    "# preview updated location count DataFrame\n",
    "location_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_HTV57PcuZz"
   },
   "source": [
    "Now that we have a `pleiades_id` for each location from names.csv, we can use that information to get more data from the places.csv. It would be possible to combine the functions below into one, but I have seperated them out for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bxf8pDtjgYHY"
   },
   "outputs": [],
   "source": [
    "def get_description(pleiades_id):\n",
    "    \"\"\"return description from a pleiades id\"\"\"\n",
    "    places_row = places_df.loc[places_df['id'] == pleiades_id]\n",
    "    if len(places_row) == 1:\n",
    "        return places_row.description.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0oV9bldghzs"
   },
   "outputs": [],
   "source": [
    "def get_uri(pleiades_id):\n",
    "    \"\"\"return uri from a pleiades id\"\"\"\n",
    "    places_row = places_df.loc[places_df['id'] == pleiades_id]\n",
    "    if len(places_row) == 1:\n",
    "        return places_row.uri.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMyIbdJEO5es"
   },
   "outputs": [],
   "source": [
    "def get_latitude(pleiades_id):\n",
    "    \"\"\"return latitude from a pleiades id\"\"\"\n",
    "    places_row = places_df.loc[places_df['id'] == pleiades_id]\n",
    "    if len(places_row) == 1:\n",
    "        return places_row.representative_latitude.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85k0-1CJP9Et"
   },
   "outputs": [],
   "source": [
    "# Challenge: Can you write a function to get the longitude data?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzV32_kjhA73"
   },
   "outputs": [],
   "source": [
    "# add new column for description\n",
    "location_count_df['description'] = location_count_df['pleiades_id'].apply(get_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnxXBkIDhRCQ"
   },
   "outputs": [],
   "source": [
    "# Challenge: can you write the code to add a colmn for the uri?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QxWuRo3QaV9"
   },
   "outputs": [],
   "source": [
    "# add new column for latitude\n",
    "location_count_df['latitude'] = location_count_df['pleiades_id'].apply(get_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ado4zh2qQkm0"
   },
   "outputs": [],
   "source": [
    "# Challenge: can you write the code to add a colmn for the longitude?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2523qXQRiDMV"
   },
   "source": [
    "Now that we have all the data we need, I am going to make a few little changes to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w0BBrQLiLCy"
   },
   "outputs": [],
   "source": [
    "# now that we have a uri we don't need the pleiades_id\n",
    "location_count_df = location_count_df.drop(columns=['pleiades_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAcuS3FJhzgO"
   },
   "outputs": [],
   "source": [
    "# for our purposes we don't really need an index, so I will make the place_name column the index\n",
    "location_count_df.set_index('place_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv3-iw1ummBm"
   },
   "outputs": [],
   "source": [
    "# final sanity check\n",
    "location_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scDoF2Ducjjd"
   },
   "source": [
    "## Save location data for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnACM1L7QyEa"
   },
   "outputs": [],
   "source": [
    "# create path and file name variables\n",
    "path = # <-- set path variable (not necessary for Colab)\n",
    "file_name = # <-- set file_name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq8zPWQ8RKEf"
   },
   "outputs": [],
   "source": [
    "# save DataFrame to a .csv file\n",
    "location_count_df.to_csv(file_name) # <-- For Jupyter you may want to add path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8fjepPdRR07"
   },
   "outputs": [],
   "source": [
    "# Colab only\n",
    "files.download(file_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

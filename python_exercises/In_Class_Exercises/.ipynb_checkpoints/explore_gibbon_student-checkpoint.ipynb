{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApv2hxy_KO0"
   },
   "source": [
    "# Exploring Gibbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z4bCFyUaJ1cA"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG63QU6DIhgZ"
   },
   "source": [
    "## Pre-process text for analysis using SpaCy\n",
    "\n",
    "Before doing NLP work, most texts will need to be preprocessed in different ways. You may need to **tokenize** the text, remove stopwords, or **lemmatize** the text. What you do in pre-processing depends entirely on what your project is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pizh0wfAQkAf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m nlp\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3045039\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'parser'])\n",
    "nlp.max_length = 3045039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spaCy doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview spaCy doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular for loop to create a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to create a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Gibbon\n",
    "\n",
    "For our immediate purposes we want to convert the raw text of Gibbon (which is in the form of `strings`) to a list of **lemmas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_and_verb_lemmas(text):\n",
    "    \"\"\"Return a list of noun and verb lemmas from a string\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    noun_and_verb_tokens = [token for token in tokens if token.pos_ == 'NOUN' or token.pos_ == 'VERB']\n",
    "    noun_and_verb_lemmas = [noun_and_verb_token.lemma_ for noun_and_verb_token in noun_and_verb_tokens]\n",
    "    return noun_and_verb_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to memory issues with spaCy it was necessary for me to find the longest file so that I could adjust some of SpaCy's\n",
    "# default settings.\n",
    "# You do not neet to run this code.\n",
    "text_path = \"../text/gibbon_decline_and_fall/\" # path to unzipped gibbon_decline_and_fall directory\n",
    "longest = 0\n",
    "for file_name in os.listdir(text_path):\n",
    "    with open(text_path + file_name, encoding='utf-8', mode='r') as f:\n",
    "        raw_text = f.read()\n",
    "    text_len = len(raw_text)\n",
    "    if text_len > longest:\n",
    "        longest = text_len\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 3 mintues\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"  # path to unzipped gibbon_decline_and_fall directory\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "    gibbon_lemmas[chapter_name] = lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4DhizTlQQOG"
   },
   "outputs": [],
   "source": [
    "# Attempt 2: This block of code is an alternative way to handle the memory limitations for spaCy.\n",
    "# You do not need to run this code.\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    if len(raw_text) < 1000000:  # SpaCy will throw a memory error if a text is more than 1,000,000 characters\n",
    "        lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "        gibbon_lemmas[chapter_name] = lemmas\n",
    "    else:\n",
    "        print(f\"Long chapter: {chapter_name}\")\n",
    "        lemmas = []\n",
    "        text_lines = raw_text.split('\\n')\n",
    "        for text_line in text_lines:\n",
    "            line_lemmas = get_noun_and_verb_lemmas(text_line)\n",
    "            for line_lemma in line_lemmas:\n",
    "                lemmas.append(line_lemma)\n",
    "        gibbon_lemmas[chapter_name] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I752weTCKR-l",
    "outputId": "ebff87dd-32b7-4882-d379-27b290d47647"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will save the data as a json file\n",
    "file_name = 'gibbon_lemmas.json'\n",
    "with open(file_name, encoding='utf-8', mode='w') as f:\n",
    "    json.dump(gibbon_lemmas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-i5QjIaJfd-"
   },
   "source": [
    "## Find the most important words by chapter in Gibbon\n",
    "For this part we are going to use a library called [scikit-learn](https://scikit-learn.org/stable/). This library is primarily for machine learning, but many of its features are useful for DH work.\n",
    "Advanced Reading: https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTGExuH9YyOv"
   },
   "outputs": [],
   "source": [
    "# The tool I will use here requires a string as input rather than a list, so I convert my docs from lists to strings\n",
    "gibbon_chap_strings = []\n",
    "gibbon_chap_names = []\n",
    "for key, value in gibbon_lemmas.items():\n",
    "    gibbon_chap_names.append(key)  \n",
    "    chap_string = ' '.join(value)\n",
    "    gibbon_chap_strings.append(chap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjctTms2WWar"
   },
   "outputs": [],
   "source": [
    "# transform corpus into a matrix of word counts\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, \n",
    "                             use_idf= True, norm=None)\n",
    "transformed_chaps = vectorizer.fit_transform(gibbon_chap_strings)\n",
    "transformed_chaps_as_array = transformed_chaps.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFR-PdNBWyhA"
   },
   "outputs": [],
   "source": [
    "gibbon_key_vocab_by_chap = {}\n",
    "for chap, chap_name in zip(transformed_chaps_as_array, gibbon_chap_names):\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), chap))\n",
    "    sorted_tf_idf_tuples = sorted(tf_idf_tuples, key= lambda x: x[1], reverse=True)\n",
    "    k = chap_name\n",
    "    v = sorted_tf_idf_tuples[:10]  # only getting the top ten\n",
    "    gibbon_key_vocab_by_chap[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOtAg3yrN2n6",
    "outputId": "e8f042ad-2d51-4736-e23d-6b8f6a218c35"
   },
   "outputs": [],
   "source": [
    "for k, v in gibbon_key_vocab_by_chap.items():\n",
    "    result = k + ' => ' + v[0][0] + ', ' + v[1][0] + ', ' + v[2][0] + ', ' + v[3][0] + ', ' + v[4][0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujc-mmSMZohr",
    "outputId": "56b3350d-d692-471b-cb0e-88ca45b6c45e"
   },
   "outputs": [],
   "source": [
    "# explore vocabulary\n",
    "gibbon_key_vocab_by_chap['chap16']  # <-- you can investigate other chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0thCE3jvaVqp"
   },
   "source": [
    "## Conditional frequency distribution in Gibbon\n",
    "\n",
    "### Natural Language Toolkit\n",
    "The **Natural Language Toolkit** (NLTK) is a library used for natural language processing (NLP). If you want to learn more, I highly recommend working through the [NLTK Book](https://www.nltk.org/book/). This resource is a great introduction to NLP specifically and Python more generally.\n",
    "\n",
    "A **conditional frequency distribution** (cfd) is a collection of word counts for a given condition, i.e. category. Here the category is separate chapters in Gibbon. We can chart what used are used most frequently by chapter. This will tell us something about the nature of each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp-vlGgoMaFL",
    "outputId": "abf6549d-22ef-49bc-c588-c6e5dcc9d532"
   },
   "outputs": [],
   "source": [
    "# conditional frequency distribution\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, chap_name)\n",
    "    for chap_name in gibbon_lemmas.keys()\n",
    "    for lemma in gibbon_lemmas[chap_name]\n",
    "    for target in ['doctrine', 'apostle', 'presbyter', 'daemon', 'immortality']  # <-- instert token(s) to explore (lowercase)\n",
    "    if lemma.lower().startswith(target)\n",
    ")\n",
    "# display plot\n",
    "plt.figure(figsize=(20, 8))  # this expands the plot to make it more readable\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih3TwGldGhEG"
   },
   "source": [
    "### Activity\n",
    "Based on the key vocabulary by chapter above, explore the use of different terms in the conditional frequency distribution. \n",
    "* What questions about the text does this raise for you?\n",
    "* What hypotheses about the text can you form?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_gibbon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

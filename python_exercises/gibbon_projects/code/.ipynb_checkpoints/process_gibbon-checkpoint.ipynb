{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApv2hxy_KO0"
   },
   "source": [
    "# Exploring Gibbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z4bCFyUaJ1cA"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import scattertext as st\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG63QU6DIhgZ"
   },
   "source": [
    "## Pre-process text for analysis using SpaCy\n",
    "\n",
    "Before doing NLP work, most texts will need to be preprocessed in different ways. You may need to **tokenize** the text, remove stopwords, or **lemmatize** the text. What you do in pre-processing depends entirely on what your project is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pizh0wfAQkAf"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'parser'])\n",
    "nlp.max_length = 3045039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"IN the second century of the Christian era, the Empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Gibbon\n",
    "\n",
    "For our immediate purposes we want to convert the raw text of Gibbon (which is in the form of `strings`) to a list of **lemmas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_and_verb_lemmas(text):\n",
    "    \"\"\"Return a list of noun and verb lemmas from a string\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    noun_and_verb_tokens = [token for token in tokens if token.pos_ == 'NOUN' or token.pos_ == 'VERB']\n",
    "    noun_and_verb_lemmas = [noun_and_verb_token.lemma_ for noun_and_verb_token in noun_and_verb_tokens]\n",
    "    return noun_and_verb_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "longest = 0\n",
    "for file_name in os.listdir(text_path):\n",
    "    with open(text_path + file_name, encoding='utf-8', mode='r') as f:\n",
    "        raw_text = f.read()\n",
    "    text_len = len(raw_text)\n",
    "    if text_len > longest:\n",
    "        longest = text_len\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 3 mintues\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "    gibbon_lemmas[chapter_name] = lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4DhizTlQQOG"
   },
   "outputs": [],
   "source": [
    "# Attempt 2: \n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    if len(raw_text) < 1000000:  # SpaCy will throw a memory error if a text is more than 1,000,000 characters\n",
    "        lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "        gibbon_lemmas[chapter_name] = lemmas\n",
    "    else:\n",
    "        print(f\"Long chapter: {chapter_name}\")\n",
    "        lemmas = []\n",
    "        text_lines = raw_text.split('\\n')\n",
    "        for text_line in text_lines:\n",
    "            line_lemmas = get_noun_and_verb_lemmas(text_line)\n",
    "            for line_lemma in line_lemmas:\n",
    "                lemmas.append(line_lemma)\n",
    "        gibbon_lemmas[chapter_name] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I752weTCKR-l",
    "outputId": "ebff87dd-32b7-4882-d379-27b290d47647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "dict_keys(['chap01', 'chap02', 'chap03', 'chap04', 'chap05', 'chap06', 'chap07', 'chap08', 'chap09', 'chap10', 'chap11', 'chap12', 'chap13', 'chap14', 'chap15', 'chap16', 'chap17', 'chap18', 'chap19', 'chap20', 'chap21', 'chap22', 'chap23', 'chap24', 'chap25', 'chap26', 'chap27', 'chap28', 'chap29', 'chap30', 'chap31', 'chap32', 'chap33', 'chap34', 'chap35', 'chap36', 'chap37', 'chap38', 'chap39', 'chap40', 'chap41', 'chap42', 'chap43', 'chap44', 'chap45', 'chap46', 'chap47', 'chap48', 'chap49', 'chap50', 'chap51', 'chap52', 'chap53', 'chap54', 'chap55', 'chap56', 'chap57', 'chap58', 'chap59', 'chap60', 'chap61', 'chap62', 'chap63', 'chap64', 'chap65', 'chap66', 'chap67', 'chap68', 'chap69', 'chap70', 'chap71'])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(len(gibbon_lemmas))\n",
    "#print(gibbon_lemmas['chap02'])\n",
    "print(gibbon_lemmas.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'gibbon_lemmas.json'\n",
    "with open(file_name, encoding='utf-8', mode='w') as f:\n",
    "    json.dump(gibbon_lemmas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-i5QjIaJfd-"
   },
   "source": [
    "## Find the most important words by chapter in Gibbon\n",
    "For this part we are going to use a library called [scikit-learn](https://scikit-learn.org/stable/). This library is primarily for machine learning, but many of its features are useful for DH work.\n",
    "Advanced Reading: https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTGExuH9YyOv"
   },
   "outputs": [],
   "source": [
    "# The tool I will use here requires a string as input rather than a list, so I convert my docs from lists to strings\n",
    "gibbon_chap_strings = []\n",
    "gibbon_chap_names = []\n",
    "for key, value in gibbon_lemmas.items():\n",
    "    gibbon_chap_names.append(key)  \n",
    "    chap_string = ' '.join(value)\n",
    "    gibbon_chap_strings.append(chap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjctTms2WWar"
   },
   "outputs": [],
   "source": [
    "# transform corpus into a matrix of word counts\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, \n",
    "                             use_idf= True, norm=None)\n",
    "transformed_chaps = vectorizer.fit_transform(gibbon_chap_strings)\n",
    "transformed_chaps_as_array = transformed_chaps.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFR-PdNBWyhA"
   },
   "outputs": [],
   "source": [
    "gibbon_key_vocab_by_chap = {}\n",
    "for chap, chap_name in zip(transformed_chaps_as_array, gibbon_chap_names):\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), chap))\n",
    "    sorted_tf_idf_tuples = sorted(tf_idf_tuples, key= lambda x: x[1], reverse=True)\n",
    "    k = chap_name\n",
    "    v = sorted_tf_idf_tuples[:10]  # only getting the top ten\n",
    "    gibbon_key_vocab_by_chap[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOtAg3yrN2n6",
    "outputId": "e8f042ad-2d51-4736-e23d-6b8f6a218c35"
   },
   "outputs": [],
   "source": [
    "for k, v in gibbon_key_vocab_by_chap.items():\n",
    "    result = k + ' => ' + v[0][0] + ', ' + v[1][0] + ', ' + v[2][0] + ', ' + v[3][0] + ', ' + v[4][0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujc-mmSMZohr",
    "outputId": "56b3350d-d692-471b-cb0e-88ca45b6c45e"
   },
   "outputs": [],
   "source": [
    "# explore vocabulary\n",
    "gibbon_key_vocab_by_chap['chap01']  # <-- you can investigate other chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0thCE3jvaVqp"
   },
   "source": [
    "## Conditional frequency distribution in Gibbon\n",
    "\n",
    "### Natural Language Toolkit\n",
    "The **Natural Language Toolkit** (NLTK) is a library used for natural language processing (NLP). If you want to learn more, I highly recommend working through the [NLTK Book](https://www.nltk.org/book/). This resource is a great introduction to NLP specifically and Python more generally.\n",
    "\n",
    "A **conditional frequency distribution** (cfd) is a collection of word counts for a given condition, i.e. category. Here the category is separate chapters in Gibbon. We can chart what used are used most frequently by chapter. This will tell us something about the nature of each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp-vlGgoMaFL",
    "outputId": "abf6549d-22ef-49bc-c588-c6e5dcc9d532"
   },
   "outputs": [],
   "source": [
    "# conditional frequency distribution\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, chap_name)\n",
    "    for chap_name in gibbon_lemmas.keys()\n",
    "    for lemma in gibbon_lemmas[chap_name]\n",
    "    for target in ['doctrine', 'apostle', 'presbyter', 'daemon', 'immortality']  # <-- instert token(s) to explore (lowercase)\n",
    "    if lemma.lower().startswith(target)\n",
    ")\n",
    "# display plot\n",
    "plt.figure(figsize=(20, 8))  # this expands the plot to make it more readable\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih3TwGldGhEG"
   },
   "source": [
    "### Activity\n",
    "Based on the key vocabulary by chapter above, explore the use of different terms in the conditional frequency distribution. \n",
    "* What questions about the text does this raise for you?\n",
    "* What hypotheses about the text can you form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxQCx7ZtQv_Y"
   },
   "source": [
    "## ScatterText\n",
    "[ScatterText](https://github.com/JasonKessler/scattertext) is a python library used to visually compare texts according to two categories.\n",
    "\n",
    "**Technical note**: Due to the large corpora we will be comparing, I have made adjustments to [spaCy](https://spacy.io/) to reduce processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW8Ot6dKtshu",
    "outputId": "1bf862ef-b2d0-4510-cfcd-e00bdde78009"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIVS4i07IPPC"
   },
   "source": [
    "### Compare Chapters in Gibbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8fUn0-_QzyK"
   },
   "outputs": [],
   "source": [
    "vol_1 = []\n",
    "for chap in gibbon_chap_strings[:16]:  # remeber: final number of a slice is exclusive\n",
    "    vol_1.append(chap)\n",
    "\n",
    "vol_6 = []\n",
    "for chap in gibbon_chap_strings[57:]:\n",
    "    vol_6.append(chap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYeA-bmMIbTW"
   },
   "source": [
    "**Note**: ScatterText requires data in a [Pandas](https://pandas.pydata.org/docs/#) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rJWE6xUT9qT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chap15_df = pd.DataFrame(data={'chapter': 'chapter 15', 'text': gibbon_lemmas['chap15']})\n",
    "chap27_df = pd.DataFrame(data={'chapter': 'chapter 27', 'text': gibbon_lemmas['chap27']})\n",
    "df = chap15_df.append(chap27_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f02QQTRV2jD"
   },
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromPandas(df, category_col='chapter', text_col='text').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Xq0QlFZV5kl",
    "outputId": "637c1488-62d8-44e6-caf9-df9456f03308"
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus, category='chapter 15',\n",
    "                                       category_name='chapter 15',\n",
    "                                       not_category_name='chapter 27',\n",
    "                                       width_in_pixels=900)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyAccvY_LIA5",
    "outputId": "c5dde59d-dec0-46d2-e488-37575254fee3"
   },
   "outputs": [],
   "source": [
    "# download interactive html\n",
    "with open('./scattertext.html', encoding='utf8', mode='w') as f:\n",
    "    f.write(html)\n",
    "#f.download('./scattertext.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f54wW8GPgXZ"
   },
   "source": [
    "## Compare Gibbon and Hume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3uc1z2WL0XY"
   },
   "source": [
    "### Prepare Hume text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiJCNRFTLyvM",
    "outputId": "7b22651b-e2ac-4951-da6f-d8855afd5b24"
   },
   "outputs": [],
   "source": [
    "hume_corpus = PlaintextCorpusReader('./18th-century-historians/hume/hume-history-of-england/', '.*\\.txt')\n",
    "print(hume_corpus.fileids()[:5])  # Just the first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-12XkdgaL8-_"
   },
   "outputs": [],
   "source": [
    "# This will take about 3 minutes\n",
    "hume_docs = []\n",
    "for fileid in hume_corpus.fileids():\n",
    "  raw_text = hume_corpus.raw(fileid)\n",
    "  lemmas = pre_process(raw_text)\n",
    "  hume_docs.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IlF0nKZN0Lb"
   },
   "outputs": [],
   "source": [
    "hume_doc_strings = []\n",
    "for doc in hume_docs:\n",
    "  string = ' '.join(doc)\n",
    "  hume_doc_strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vD74wwx0Pi4w"
   },
   "outputs": [],
   "source": [
    "gibbon_df = pd.DataFrame(data={'author': 'Gibbon', 'text': gibbon_doc_strings})\n",
    "hume_df = pd.DataFrame(data={'author': 'Hume', 'text': hume_doc_strings})\n",
    "author_df = gibbon_df.append(hume_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d1mIEjKFeo8R"
   },
   "outputs": [],
   "source": [
    "# about 3 min\n",
    "author_corpus = st.CorpusFromPandas(author_df,\n",
    "                                    category_col='author',\n",
    "                                    text_col='text',\n",
    "                                    nlp=nlp,\n",
    "                                    ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dYVnynoEiQgI"
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(author_corpus, category='Gibbon',\n",
    "                                       category_name='Gibbon',\n",
    "                                       not_category_name='Hume',\n",
    "                                       width_in_pixels=900)\n",
    "HTML(html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_gibbon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
